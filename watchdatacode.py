import os
import argparse
import pandas as pd
import shutil
import time
from datetime import datetime
from tensorboard.backend.event_processing.event_accumulator import EventAccumulator


def extract_scalars_to_csv(event_file_path, base_output_dir):
    """
    è¯»å– TensorBoard äº‹ä»¶æ–‡ä»¶ï¼Œå¯¼å‡º CSVï¼Œå¹¶è‡ªåŠ¨å¤‡ä»½æºæ–‡ä»¶åˆ°å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å¤¹ä¸­ã€‚
    """
    # 1. æ£€æŸ¥æºæ–‡ä»¶
    if not os.path.exists(event_file_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {event_file_path}")
        return

    # 2. ç”Ÿæˆæ—¶é—´æˆ³ (ä¾‹å¦‚: 20251207_143005)
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 3. åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ä¸“å±è¾“å‡ºç›®å½•
    # ä¾‹å¦‚: ./exported_data/20251207_143005_backup
    final_output_dir = os.path.join(base_output_dir, f"{timestamp_str}_backup")

    if not os.path.exists(final_output_dir):
        os.makedirs(final_output_dir)
        print(f"âœ… å·²åˆ›å»ºä»»åŠ¡ç›®å½•: {final_output_dir}")

    # 4. å¤‡ä»½æºæ–‡ä»¶ (Raw Data Backup)
    print(f"ğŸ“¦ æ­£åœ¨å¤‡ä»½åŸå§‹æ•°æ®æ–‡ä»¶...")

    # (A) å¤‡ä»½ tfevents äºŒè¿›åˆ¶æ–‡ä»¶
    try:
        shutil.copy2(event_file_path, final_output_dir)
        print(f"   - å·²å¤‡ä»½: {os.path.basename(event_file_path)}")
    except Exception as e:
        print(f"   - å¤‡ä»½ tfevents æ–‡ä»¶å¤±è´¥: {e}")

    # (B) å¤‡ä»½åŒç›®å½•ä¸‹çš„ log.txt
    source_dir = os.path.dirname(event_file_path)
    log_txt_path = os.path.join(source_dir, "log.txt")

    if os.path.exists(log_txt_path):
        try:
            shutil.copy2(log_txt_path, final_output_dir)
            print(f"   - å·²å¤‡ä»½: log.txt")
        except Exception as e:
            print(f"   - å¤‡ä»½ log.txt å¤±è´¥: {e}")
    else:
        print(f"   - æç¤º: æœªåœ¨æºç›®å½•æ‰¾åˆ° log.txtï¼Œè·³è¿‡å¤‡ä»½ã€‚")

    print("-" * 50)
    print(f"ğŸš€ æ­£åœ¨è§£æ TensorBoard æ•°æ®... (æ–‡ä»¶è¾ƒå¤§æ—¶è¯·è€å¿ƒç­‰å¾…)")

    # 5. åŠ è½½äº‹ä»¶æ–‡ä»¶
    ea = EventAccumulator(event_file_path, size_guidance={'scalars': 0})
    ea.Reload()

    # 6. è·å–æ‰€æœ‰æ ‡é‡æ ‡ç­¾
    tags = ea.Tags()['scalars']

    if not tags:
        print("âŒ æœªåœ¨æ–‡ä»¶ä¸­æ‰¾åˆ°ä»»ä½•æ ‡é‡æ•°æ® (Scalars/æ›²çº¿å›¾)ã€‚")
        return

    print(f"ğŸ“Š æ‰¾åˆ° {len(tags)} æ¡æ›²çº¿æ•°æ®ï¼Œå¼€å§‹å¯¼å‡º CSV...")

    count = 0
    for tag in tags:
        # è·å–æ•°æ®
        events = ea.Scalars(tag)

        steps = [x.step for x in events]
        values = [x.value for x in events]
        wall_times = [x.wall_time for x in events]

        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame({
            'step': steps,
            'value': values,
            'wall_time': wall_times
        })

        # 7. ç”Ÿæˆæ–‡ä»¶å (å¸¦æ—¶é—´æˆ³)
        # å°† tag ä¸­çš„ '/' æ›¿æ¢ä¸º '_', å¹¶åŠ ä¸Šæ—¶é—´åç¼€
        # ä¾‹å¦‚: data_r_soc_limit_20251207_143005.csv
        clean_tag_name = tag.replace('/', '_').replace('\\', '_')
        filename = f"{clean_tag_name}_{timestamp_str}.csv"
        output_path = os.path.join(final_output_dir, filename)

        # ä¿å­˜
        df.to_csv(output_path, index=False)
        print(f"   - å¯¼å‡º: {filename}")
        count += 1

    print("-" * 50)
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“‚ ç»“æœå·²ä¿å­˜åœ¨: {os.path.abspath(final_output_dir)}")
    print(f"   åŒ…å«: {count} ä¸ª CSV æ•°æ®è¡¨ + åŸå§‹æ•°æ®å¤‡ä»½")


if __name__ == '__main__':
    # é…ç½®éƒ¨åˆ†

    # é»˜è®¤è¾“å‡ºæ€»ç›®å½• (è„šæœ¬ä¼šåœ¨è¿™ä¸ªç›®å½•ä¸‹è‡ªåŠ¨æ–°å»ºå¸¦æ—¶é—´æˆ³çš„å­ç›®å½•)
    DEFAULT_OUTPUT_ROOT = "./exported_data"

    # é»˜è®¤æ—¥å¿—æ–‡ä»¶è·¯å¾„ (ä½ å¯ä»¥æ”¹æˆä½ çš„è·¯å¾„ï¼Œæˆ–è€…è®©è„šæœ¬è‡ªåŠ¨æœ)
    # è¿™é‡Œå†™ç›¸å¯¹è·¯å¾„å³å¯
    # DEFAULT_LOG_DIR = "debug_logs/tensorboard"
    DEFAULT_LOG_DIR = "results/tensorboard/var_voltage_control-case33_3min_final-distributed-matd3-l1-production_run_v1_1207"
    parser = argparse.ArgumentParser(description="Export TensorBoard events to CSV with Backup.")
    parser.add_argument("--log_dir", type=str, default=DEFAULT_LOG_DIR, help="Directory containing the tfevents file")
    parser.add_argument("--output_dir", type=str, default=DEFAULT_OUTPUT_ROOT, help="Root directory to save exports")

    args = parser.parse_args()

    # è‡ªåŠ¨å¯»æ‰¾æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
    target_log_file = None

    # å¦‚æœç”¨æˆ·ç»™çš„æ˜¯ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥ç”¨
    if os.path.isfile(args.log_dir):
        target_log_file = args.log_dir
    # å¦‚æœç»™çš„æ˜¯ç›®å½•ï¼Œå»é‡Œé¢æ‰¾æœ€æ–°çš„ tfevents
    elif os.path.isdir(args.log_dir):
        print(f"ğŸ” æ­£åœ¨ç›®å½• '{args.log_dir}' ä¸­æœç´¢æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶...")
        all_files = []
        for root, dirs, files in os.walk(args.log_dir):
            for file in files:
                if "events.out.tfevents" in file:
                    full_path = os.path.join(root, file)
                    all_files.append(full_path)

        if all_files:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            target_log_file = max(all_files, key=os.path.getmtime)
            print(f"ğŸ‘‰ è‡ªåŠ¨é€‰ä¸­æœ€æ–°æ–‡ä»¶: {target_log_file}")
        else:
            print(f"âŒ é”™è¯¯: åœ¨ '{args.log_dir}' ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½• tfevents æ–‡ä»¶ã€‚")
            exit(1)
    else:
        print(f"âŒ é”™è¯¯: è·¯å¾„ '{args.log_dir}' ä¸å­˜åœ¨ã€‚")
        exit(1)

    # æ‰§è¡Œä¸»é€»è¾‘
    extract_scalars_to_csv(target_log_file, args.output_dir)